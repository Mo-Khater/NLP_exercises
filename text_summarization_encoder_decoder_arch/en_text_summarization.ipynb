{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9812e015",
   "metadata": {},
   "source": [
    "# load summerization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54359c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101c697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"billsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19048ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 18949\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "    ca_test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 1237\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca79f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f957fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5bc9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561\n"
     ]
    }
   ],
   "source": [
    "print(len(train['summary'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8e4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7173cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data,sos_token,eos_token):\n",
    "    text_tokens = nltk.word_tokenize(data['text'])\n",
    "    summarized_tokens = nltk.word_tokenize(data['summary'])\n",
    "    text_tokens = [sos_token] + text_tokens + [eos_token]\n",
    "    summarized_tokens = [sos_token] + summarized_tokens + [eos_token]\n",
    "    return {'text_tokens':text_tokens,'summary_tokens':summarized_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ede55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\n",
    "    'sos_token':sos_token,\n",
    "    'eos_token':eos_token,\n",
    "}\n",
    "train = train.map(tokenizer,fn_kwargs=fn_kwargs)\n",
    "test = test.map(tokenizer,fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26b0614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2954"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['text_tokens'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afe9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "def build_vocab(sentences):\n",
    "    # counter = Counter([token for tokens in sentences for token in tokens])\n",
    "    idx = 2  \n",
    "    vocab = {'<unk>':0,'<pad>':1}\n",
    "    for tokens in sentences:\n",
    "        for token in tokens:\n",
    "            if token in vocab.keys():\n",
    "                continue\n",
    "            vocab[token] = idx \n",
    "            idx += 1\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fd219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq,pad,max_length):\n",
    "    if(len(seq)>max_length):\n",
    "        return seq[:max_length]\n",
    "    return seq + [pad]*(max_length-len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f21a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word2index(vocab,tokens):\n",
    "    tokens_indexes = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            tokens_indexes.append(vocab[token])\n",
    "        else:\n",
    "            tokens_indexes.append(vocab['<unk>'])\n",
    "    return tokens_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13bf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['text_tokens']\n",
    "summaries = train['summary_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82118dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e217c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [convert_word2index(vocab,pad_seq(tokens,'<pad>',2000)) for tokens in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee28f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af96ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [convert_word2index(vocab,pad_seq(tokens,'<pad>',1000)) for tokens in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75cc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['text_tokens']\n",
    "test_summary = test['summary_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0802a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_idxes = [convert_word2index(vocab,pad_seq(tokens,'<pad>',2000)) for tokens in test_text]\n",
    "test_summary_idxes = [convert_word2index(vocab,pad_seq(tokens,'<pad>',1000)) for tokens in test_summary]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0aaec",
   "metadata": {},
   "source": [
    "# DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7c3244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a408a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummary(Dataset):\n",
    "    def __init__(self,text_idxes,summaries_idxs):\n",
    "        self.text_idxes = text_idxes \n",
    "        self.summaries_idxs = summaries_idxs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_idxes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.text_idxes[index], dtype=torch.long), torch.tensor(self.summaries_idxs[index], dtype=torch.long)\n",
    "\n",
    "    \n",
    "\n",
    "text_summary = TextSummary(text_idxes = texts , summaries_idxs = summaries)\n",
    "train_dataloader = DataLoader(text_summary , batch_size=32 , shuffle= True)\n",
    "test_text_summary = TextSummary(text_idxes= test_text_idxes,summaries_idxs=test_summary_idxes)\n",
    "test_dataloader = DataLoader(test_text_summary,batch_size=32,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177eef6",
   "metadata": {},
   "source": [
    "# building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffb7bb",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e0e8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim,dropout,num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_dim,embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,dropout=dropout)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        embedding_input = self.dropout(self.embedding(X))\n",
    "        outputs,(hidden,cell) = self.lstm(embedding_input)\n",
    "        return hidden,cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640c688",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caccac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,output_dim,embedding_dim,hidden_dim,dropout,num_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embbedding = torch.nn.Embedding(output_dim,embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,dropout=dropout)\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,X,prev_hidden,prev_cell):\n",
    "        embedding_input = self.dropout(self.embedding(X.unsqueeze(0)))\n",
    "        outputs,(hidden,cell) = self.lstm(embedding_input)\n",
    "        prediction = self.fc1(outputs.squeeze(0))\n",
    "        return prediction,hidden,cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c59ef",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b40c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,X,target,teaching_force_ratio):\n",
    "        hidden,cell = self.encoder(X)\n",
    "        decoder_input = target[0,:]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        batch_size = target.shape[1]\n",
    "        target_size = target.shape[0]\n",
    "        outputs = torch.zeros(target_size,batch_size,output_dim).to(self.device)\n",
    "\n",
    "        for t in range(1,len(target_size)):\n",
    "            output,hidden,cell = self.decoder(decoder_input,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            teaching_force = random.random() < teaching_force_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target[t] if teaching_force else top1 \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c15322",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8603d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "output_dim = len(vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "encoder_dropout = .5\n",
    "decoder_dropout = .5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim,embedding_dim = encoder_embedding_dim,hidden_dim=hidden_dim,dropout=encoder_dropout,num_layers=num_layers)\n",
    "decoder = Decoder(output_dim=output_dim,embedding_dim=decoder_embedding_dim,hidden_dim=hidden_dim,dropout=decoder_dropout,num_layers=num_layers)\n",
    "model = Seq2Seq(encoder=encoder,decoder=decoder,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0b55c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(131499, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embbedding): Embedding(131499, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc1): Linear(in_features=512, out_features=131499, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        torch.nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88da47c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 142,142,891 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cef686a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\__init__.py\n",
      "2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__file__)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc5df7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m criterion = torch.nn.CrossEntropyLoss(ignore_index=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:100\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     88\u001b[39m     lr=lr,\n\u001b[32m     89\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:369\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    366\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_dynamo\\exc.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_dynamo\\utils.py:68\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal, TypeIs\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_functorch\\config.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Applies CSE to the graph before partitioning\u001b[39;00m\n\u001b[32m     38\u001b[39m cse = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fbcode\n\u001b[32m     43\u001b[39m enable_autograd_cache: \u001b[38;5;28mbool\u001b[39m = Config(\n\u001b[32m     44\u001b[39m     justknob=\u001b[33m\"\u001b[39m\u001b[33mpytorch/remote_cache:enable_local_autograd_cache\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     env_name_force=\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_AUTOGRAD_CACHE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     default=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremote_autograd_cache_default\u001b[39m() -> Optional[\u001b[38;5;28mbool\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_inductor\\__init__.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, IO, Optional, TYPE_CHECKING, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\_inductor\\config.py:202\u001b[39m\n\u001b[32m    195\u001b[39m b2b_gemm_pass = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# register custom graph optimization pass hook. so far, pre/post passes are\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# only applied before/after pattern_matcher in post_grad_passes.\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Implement CustomGraphPass to allow Inductor to graph compiled artifacts\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# to which your custom passes have been applied:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m post_grad_custom_pre_pass: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_inductor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_graph_pass\u001b[49m.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Registers a custom joint graph pass.\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch._inductor' has no attribute 'custom_graph_pass' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c037a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model,optimizer,criterian,teaching_force_ratio,data_loader,clip,device):\n",
    "    epoch_loss = 0 \n",
    "    model.train()\n",
    "    for X_batch,y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_batch.T \n",
    "        y_batch = y_batch.T\n",
    "        output = model(X_batch.to(device),y_batch.to(device),teaching_force_ratio)\n",
    "        output = output[1:].view(-1,output.shape[-1])\n",
    "        y_batch = y_batch[1:].view(-1)\n",
    "\n",
    "        loss = criterian(output,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_fn(model,test_loader,criterian,device):\n",
    "    epoch_loss = 0 \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_batch,y_batch in test_loader:\n",
    "            X_batch = X_batch.T \n",
    "            y_batch = y_batch.T\n",
    "            output = model(X_batch.to(device),y_batch.to(device),0)\n",
    "            output = output[1:].view(-1,output.shape[-1])\n",
    "            y_batch = y_batch[1:].view(-1)\n",
    "            loss = criterian(output,y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss/test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "teaching_force_ratio = .5\n",
    "clip =1\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    train_loss = train_fn(model,optimizer,criterion,teaching_force_ratio,train_dataloader,clip,device)\n",
    "    test_loss = evalute_fn(model,test_dataloader,criterion,device)\n",
    "    if test_loss < best_valid_loss:\n",
    "        best_valid_loss = best_valid_loss\n",
    "        torch.save(model.state_dict(), \"text_summarization.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {best_valid_loss:7.3f} | Valid PPL: {np.exp(best_valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f63d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Index2Word(vocab):\n",
    "    index2word = {}\n",
    "    for word,idx in vocab.items():\n",
    "        index2word[idx] =word\n",
    "    return index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = Index2Word(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1ad6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5501f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_fn(model,sentence,vocab,max_length,index2word):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    indexes = convert_word2index(vocab,tokens)\n",
    "    tensor = torch.tensor(indexes,dtype=torch.int)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    hidden,cell = model.encoder(tensor)\n",
    "    input = torch.tensor([vocab[sos_token]],dtype=torch.int)\n",
    "    outputs = []\n",
    "    for _ in range(max_length):\n",
    "        prediction,hidden,cell = model.decoder(input)\n",
    "        predicted_idx = prediction.argmax(-1).item()\n",
    "        input = torch.tensor([predicted_idx],dtype=torch.int)\n",
    "        outputs.append(index2word[predicted_idx])\n",
    "        if(predicted_idx == vocab[eos_token]):\n",
    "            break\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c834531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
