{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9812e015",
   "metadata": {},
   "source": [
    "# load summerization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54359c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101c697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"billsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19048ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 18949\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 3269\n",
       "    })\n",
       "    ca_test: Dataset({\n",
       "        features: ['text', 'summary', 'title'],\n",
       "        num_rows: 1237\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca79f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f957fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5bc9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561\n"
     ]
    }
   ],
   "source": [
    "print(len(train['summary'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8e4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7173cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data,sos_token,eos_token):\n",
    "    text_tokens = nltk.word_tokenize(data['text'])\n",
    "    summarized_tokens = nltk.word_tokenize(data['summary'])\n",
    "    text_tokens = [sos_token] + text_tokens + [eos_token]\n",
    "    summarized_tokens = [sos_token] + summarized_tokens + [eos_token]\n",
    "    return {'text_tokens':text_tokens,'summary_tokens':summarized_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ede55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\n",
    "    'sos_token':sos_token,\n",
    "    'eos_token':eos_token,\n",
    "}\n",
    "train = train.map(tokenizer,fn_kwargs=fn_kwargs)\n",
    "test = test.map(tokenizer,fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26b0614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2954"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['text_tokens'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0afe9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "def build_vocab(sentences):\n",
    "    # counter = Counter([token for tokens in sentences for token in tokens])\n",
    "    idx = 2  \n",
    "    vocab = {'<unk>':0,'<pad>':1}\n",
    "    for tokens in sentences:\n",
    "        for token in tokens:\n",
    "            if token in vocab.keys():\n",
    "                continue\n",
    "            vocab[token] = idx \n",
    "            idx += 1\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq,pad,max_length):\n",
    "    if(len(seq)>max_length):\n",
    "        return seq[:max_length]\n",
    "    return seq + [pad]*(max_length-len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f21a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word2index(vocab,tokens):\n",
    "    tokens_indexes = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            tokens_indexes.append(vocab[token])\n",
    "        else:\n",
    "            tokens_indexes.append(vocab['<unk>'])\n",
    "    return tokens_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d13bf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train['text_tokens'][:1000]\n",
    "summaries = train['summary_tokens'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82118dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e217c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [convert_word2index(vocab,pad_seq(tokens,'<pad>',1000)) for tokens in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee28f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af96ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [convert_word2index(vocab,pad_seq(tokens,'<pad>',500)) for tokens in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b75cc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test['text_tokens']\n",
    "test_summary = test['summary_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0802a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_idxes = [convert_word2index(vocab,pad_seq(tokens,'<pad>',1000)) for tokens in test_text]\n",
    "test_summary_idxes = [convert_word2index(vocab,pad_seq(tokens,'<pad>',500)) for tokens in test_summary]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0aaec",
   "metadata": {},
   "source": [
    "# DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c3244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5a408a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummary(Dataset):\n",
    "    def __init__(self,text_idxes,summaries_idxs):\n",
    "        self.text_idxes = text_idxes \n",
    "        self.summaries_idxs = summaries_idxs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_idxes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.text_idxes[index], dtype=torch.long), torch.tensor(self.summaries_idxs[index], dtype=torch.long)\n",
    "\n",
    "    \n",
    "\n",
    "text_summary = TextSummary(text_idxes = texts , summaries_idxs = summaries)\n",
    "train_dataloader = DataLoader(text_summary , batch_size=16 , shuffle= True)\n",
    "test_text_summary = TextSummary(text_idxes= test_text_idxes,summaries_idxs=test_summary_idxes)\n",
    "test_dataloader = DataLoader(test_text_summary,batch_size=16,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177eef6",
   "metadata": {},
   "source": [
    "# building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffb7bb",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e0e8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim,dropout,num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_dim,embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # X [batch_size,seq_len]\n",
    "        embedding_input = self.dropout(self.embedding(X))\n",
    "        # embedding_input [batch_size,seq_len,embedding_dim]\n",
    "        outputs,(hidden,cell) = self.lstm(embedding_input)\n",
    "        # outputs [batch_size,seq_length,hidden_dim]\n",
    "        # hidden [num_layers,batch_size,hidden_dim]\n",
    "        return hidden,cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640c688",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caccac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self,output_dim,embedding_dim,hidden_dim,dropout,num_layers):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = torch.nn.Embedding(output_dim,embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,X,prev_hidden,prev_cell):\n",
    "        # X [batch_size]\n",
    "        X = X.unsqueeze(1)\n",
    "        # X [batch_size,1]\n",
    "        embedding_input = self.dropout(self.embedding(X))\n",
    "        outputs,(hidden,cell) = self.lstm(embedding_input,(prev_hidden,prev_cell))\n",
    "        # outputs [batch_size,1,hidden_dim]\n",
    "        outputs = outputs.squeeze(1)\n",
    "        # outputs [batch_size,hidden_dim]\n",
    "        prediction = self.fc1(outputs)\n",
    "        # prediction = [batch_size,output_dim]\n",
    "        return prediction,hidden,cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c59ef",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b40c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,X,target,teaching_force_ratio):\n",
    "        # X [batch_size,seq_length]\n",
    "        # target [batch_size,seq_length]\n",
    "        hidden,cell = self.encoder(X)\n",
    "        # hidden [num_layers,batch_size,hidden_dim]\n",
    "        # cell [num_layers,batch_size,hidden_dim]\n",
    "        decoder_input = target[:,0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        batch_size = target.shape[0]\n",
    "        target_size = target.shape[1]\n",
    "        outputs = torch.zeros(target_size,batch_size,output_dim).to(self.device)\n",
    "        # outputs [seq_length , batch_size,output_dim]\n",
    "        for t in range(1,target_size):\n",
    "            output,hidden,cell = self.decoder(decoder_input,hidden,cell)\n",
    "            # output [batch_size,output_dim]\n",
    "            outputs[t] = output\n",
    "            teaching_force = random.random() < teaching_force_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target[:,t] if teaching_force else top1 \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c15322",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8603d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in dispatch_shell\n",
      "    await result\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 752, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23040\\3310857315.py\", line 10, in <module>\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23040\\3310857315.py:10: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(vocab)\n",
    "output_dim = len(vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "encoder_dropout = .5\n",
    "decoder_dropout = .5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim,embedding_dim = encoder_embedding_dim,hidden_dim=hidden_dim,dropout=encoder_dropout,num_layers=num_layers)\n",
    "decoder = Decoder(output_dim=output_dim,embedding_dim=decoder_embedding_dim,hidden_dim=hidden_dim,dropout=decoder_dropout,num_layers=num_layers)\n",
    "model = Seq2Seq(encoder=encoder,decoder=decoder,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0b55c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(33728, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(33728, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc1): Linear(in_features=512, out_features=33728, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        torch.nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88da47c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 41,927,616 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cef686a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\NLP_exercises\\text_summarization_encoder_decoder_arch\\venv\\Lib\\site-packages\\torch\\__init__.py\n",
      "2.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__file__)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc5df7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c037a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model,optimizer,criterian,teaching_force_ratio,data_loader,clip,device):\n",
    "    epoch_loss = 0 \n",
    "    model.train()\n",
    "    for X_batch,y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device),y_batch.to(device),teaching_force_ratio)\n",
    "        # output [seq_length,batch_size,output_dim]\n",
    "        output = output[1:, :, :].reshape(-1, output.shape[-1])\n",
    "        y_batch = y_batch[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterian(output,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77c5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_fn(model,test_loader,criterian,device):\n",
    "    epoch_loss = 0 \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_batch,y_batch in test_loader:\n",
    "            output = model(X_batch.to(device),y_batch.to(device),0)\n",
    "\n",
    "            output = output[1:, :, :].reshape(-1, output.shape[-1])\n",
    "            y_batch = y_batch[:, 1:].reshape(-1)\n",
    "            loss = criterian(output,y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss/test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ab11c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m clip =\u001b[32m1\u001b[39m\n\u001b[32m      4\u001b[39m best_valid_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m.tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[32m      6\u001b[39m     train_loss = train_fn(model,optimizer,criterion,teaching_force_ratio,train_dataloader,clip,device)\n\u001b[32m      7\u001b[39m     test_loss = evalute_fn(model,test_dataloader,criterion,device)\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "teaching_force_ratio = .5\n",
    "clip =1\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    train_loss = train_fn(model,optimizer,criterion,teaching_force_ratio,train_dataloader,clip,device)\n",
    "    test_loss = evalute_fn(model,test_dataloader,criterion,device)\n",
    "    if test_loss < best_valid_loss:\n",
    "        best_valid_loss = best_valid_loss\n",
    "        torch.save(model.state_dict(), \"text_summarization.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {best_valid_loss:7.3f} | Valid PPL: {np.exp(best_valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f63d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Index2Word(vocab):\n",
    "    index2word = {}\n",
    "    for word,idx in vocab.items():\n",
    "        index2word[idx] =word\n",
    "    return index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = Index2Word(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1ad6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5501f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_fn(model,sentence,vocab,max_length,index2word):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    indexes = convert_word2index(vocab,tokens)\n",
    "    tensor = torch.tensor(indexes,dtype=torch.int)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    hidden,cell = model.encoder(tensor)\n",
    "    input = torch.tensor([vocab[sos_token]],dtype=torch.int)\n",
    "    outputs = []\n",
    "    for _ in range(max_length):\n",
    "        prediction,hidden,cell = model.decoder(input)\n",
    "        predicted_idx = prediction.argmax(-1).item()\n",
    "        input = torch.tensor([predicted_idx],dtype=torch.int)\n",
    "        outputs.append(index2word[predicted_idx])\n",
    "        if(predicted_idx == vocab[eos_token]):\n",
    "            break\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c834531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
